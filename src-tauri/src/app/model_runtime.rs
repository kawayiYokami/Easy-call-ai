async fn call_model_openai_rig_style(
    api_config: &ResolvedApiConfig,
    model_name: &str,
    prepared: PreparedPrompt,
) -> Result<String, String> {
    let mut content_items: Vec<UserContent> = vec![UserContent::text(prepared.preamble)];

    if !prepared.latest_user_text.trim().is_empty() {
        content_items.push(UserContent::text(prepared.latest_user_text));
    }

    for (mime, bytes) in prepared.latest_images {
        content_items.push(UserContent::image_base64(
            bytes,
            image_media_type_from_mime(&mime),
            Some(ImageDetail::Auto),
        ));
    }

    for (mime, bytes) in prepared.latest_audios {
        content_items.push(UserContent::audio(bytes, audio_media_type_from_mime(&mime)));
    }

    let prompt_content = OneOrMany::many(content_items)
        .map_err(|_| "Request payload is empty. Provide text, image, or audio.".to_string())?;

    let mut client_builder: openai::ClientBuilder =
        openai::Client::builder().api_key(&api_config.api_key);
    if !api_config.base_url.is_empty() {
        client_builder = client_builder.base_url(&api_config.base_url);
    }
    let client = client_builder
        .build()
        .map_err(|err| format!("Failed to create OpenAI client via rig: {err}"))?;

    let agent = client
        .completions_api()
        .agent(model_name)
        .temperature(api_config.temperature)
        .build();
    let prompt_message = RigMessage::User {
        content: prompt_content,
    };

    agent
        .prompt(prompt_message)
        .await
        .map_err(|err| format!("rig prompt failed: {err}"))
}

fn debug_value_snippet(value: &Value, max_chars: usize) -> String {
    let raw = serde_json::to_string(value).unwrap_or_else(|_| "<invalid json>".to_string());
    if raw.chars().count() <= max_chars {
        raw
    } else {
        let head = raw.chars().take(max_chars).collect::<String>();
        format!("{head}...")
    }
}

fn send_tool_status_event(
    on_delta: &tauri::ipc::Channel<AssistantDeltaEvent>,
    tool_name: &str,
    tool_status: &str,
    message: &str,
) {
    let send_result = on_delta.send(AssistantDeltaEvent {
        delta: String::new(),
        kind: Some("tool_status".to_string()),
        tool_name: Some(tool_name.to_string()),
        tool_status: Some(tool_status.to_string()),
        message: Some(message.to_string()),
    });
    eprintln!(
        "[TOOL-DEBUG] tool_status_event send={:?} name={} status={} message={}",
        send_result, tool_name, tool_status, message
    );
}

fn tool_enabled(selected_api: &ApiConfig, id: &str) -> bool {
    selected_api.enable_tools && selected_api.tools.iter().any(|tool| tool.id == id)
}

#[derive(Debug)]
struct ToolInvokeError(String);

impl std::fmt::Display for ToolInvokeError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&self.0)
    }
}

impl std::error::Error for ToolInvokeError {}

impl From<String> for ToolInvokeError {
    fn from(value: String) -> Self {
        Self(value)
    }
}

fn clean_text(input: &str) -> String {
    input.split_whitespace().collect::<Vec<_>>().join(" ")
}

fn is_image_unsupported_error(err: &str) -> bool {
    let lower = err.to_ascii_lowercase();
    lower.contains("unknown variant `image_url`")
        || lower.contains("expected `text`")
        || lower.contains("does not support image")
        || lower.contains("image input")
}

fn truncate_by_chars(input: &str, max_chars: usize) -> String {
    if max_chars == 0 {
        return String::new();
    }
    if input.chars().count() <= max_chars {
        return input.to_string();
    }
    let mut out = String::new();
    for (idx, ch) in input.chars().enumerate() {
        if idx >= max_chars {
            break;
        }
        out.push(ch);
    }
    out.push_str("...");
    out
}

async fn builtin_fetch(url: &str, max_length: usize) -> Result<Value, String> {
    let client = reqwest::Client::builder()
        .timeout(std::time::Duration::from_secs(12))
        .build()
        .map_err(|err| format!("Build HTTP client failed: {err}"))?;
    let resp = client
        .get(url)
        .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
        .send()
        .await
        .map_err(|err| format!("Fetch url failed: {err}"))?;
    let status = resp.status();
    if !status.is_success() {
        return Err(format!("Fetch url failed with status {status}"));
    }
    let html = resp
        .text()
        .await
        .map_err(|err| format!("Read body failed: {err}"))?;
    let document = Html::parse_document(&html);
    let body_selector =
        Selector::parse("body").map_err(|err| format!("Parse selector failed: {err}"))?;
    let raw = document
        .select(&body_selector)
        .next()
        .map(|n| n.text().collect::<Vec<_>>().join(" "))
        .unwrap_or_else(|| document.root_element().text().collect::<Vec<_>>().join(" "));
    let cleaned = clean_text(&raw);
    let truncated = truncate_by_chars(&cleaned, max_length);
    Ok(serde_json::json!({
      "url": url,
      "content": truncated
    }))
}

async fn builtin_bing_search(query: &str, num_results: usize) -> Result<Value, String> {
    let client = reqwest::Client::builder()
        .timeout(std::time::Duration::from_secs(12))
        .build()
        .map_err(|err| format!("Build HTTP client failed: {err}"))?;
    let mut last_error: Option<String> = None;
    for base in ["https://cn.bing.com", "https://www.bing.com"] {
        let url = format!("{base}/search?q={}", urlencoding::encode(query));
        let resp = client
            .get(&url)
            .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
            .send()
            .await;
        let Ok(resp) = resp else {
            last_error = Some("request failed".to_string());
            continue;
        };
        if !resp.status().is_success() {
            last_error = Some(format!("status {}", resp.status()));
            continue;
        }
        let html = resp
            .text()
            .await
            .map_err(|err| format!("Read search body failed: {err}"))?;
        let doc = Html::parse_document(&html);
        let item_sel =
            Selector::parse("li.b_algo").map_err(|err| format!("Parse selector failed: {err}"))?;
        let title_sel =
            Selector::parse("h2").map_err(|err| format!("Parse selector failed: {err}"))?;
        let a_sel =
            Selector::parse("h2 a").map_err(|err| format!("Parse selector failed: {err}"))?;
        let p_sel = Selector::parse("p").map_err(|err| format!("Parse selector failed: {err}"))?;
        let mut rows = Vec::new();
        for item in doc.select(&item_sel).take(num_results.max(1)) {
            let title = item
                .select(&title_sel)
                .next()
                .map(|n| clean_text(&n.text().collect::<Vec<_>>().join(" ")))
                .unwrap_or_default();
            let link = item
                .select(&a_sel)
                .next()
                .and_then(|n| n.value().attr("href"))
                .unwrap_or_default()
                .to_string();
            let snippet = item
                .select(&p_sel)
                .next()
                .map(|n| clean_text(&n.text().collect::<Vec<_>>().join(" ")))
                .unwrap_or_default();
            if !title.is_empty() && !link.is_empty() {
                rows.push(serde_json::json!({"title": title, "url": link, "snippet": snippet}));
            }
        }
        if !rows.is_empty() {
            return Ok(serde_json::json!({"query": query, "results": rows}));
        }
        last_error = Some("no results parsed".to_string());
    }
    Err(format!(
        "bing search failed: {}",
        last_error.unwrap_or_else(|| "unknown".to_string())
    ))
}

fn normalize_memory_keywords(raw: &[String]) -> Vec<String> {
    let mut out = Vec::<String>::new();
    for item in raw {
        let v = item.trim().to_lowercase();
        if v.len() < 2 {
            continue;
        }
        if !out.iter().any(|x| x == &v) {
            out.push(v);
        }
        if out.len() >= 12 {
            break;
        }
    }
    out
}

fn memory_contains_sensitive(content: &str, keywords: &[String]) -> bool {
    let mut full = content.to_lowercase();
    if !keywords.is_empty() {
        full.push('\n');
        full.push_str(&keywords.join(" ").to_lowercase());
    }
    let danger_tokens = [
        "password",
        "passwd",
        "api key",
        "apikey",
        "token",
        "secret",
        "private key",
        "sk-",
        "ssh-rsa",
        "验证码",
        "密码",
        "密钥",
        "身份证",
        "银行卡",
        "cvv",
    ];
    danger_tokens.iter().any(|token| full.contains(token))
}

fn builtin_memory_save(app_state: &AppState, args: Value) -> Result<Value, String> {
    let content = args
        .get("content")
        .and_then(Value::as_str)
        .map(str::trim)
        .filter(|v| !v.is_empty())
        .ok_or_else(|| "memory_save.content is required".to_string())?;
    let keywords_raw = args
        .get("keywords")
        .and_then(Value::as_array)
        .ok_or_else(|| "memory_save.keywords is required".to_string())?
        .iter()
        .filter_map(Value::as_str)
        .map(|s| s.to_string())
        .collect::<Vec<_>>();
    let keywords = normalize_memory_keywords(&keywords_raw);
    if keywords.is_empty() {
        return Err("memory_save.keywords must contain at least one valid keyword".to_string());
    }
    if memory_contains_sensitive(content, &keywords) {
        eprintln!(
            "[TOOL-DEBUG] memory-save rejected sensitive content. keywords={}",
            keywords.join(",")
        );
        return Ok(serde_json::json!({
          "saved": false,
          "reason": "sensitive_rejected"
        }));
    }

    let guard = app_state
        .state_lock
        .lock()
        .map_err(|_| "Failed to lock state mutex".to_string())?;
    let mut data = read_app_data(&app_state.data_path)?;
    let now = now_iso();
    let memory_id = if let Some(existing) = data
        .memories
        .iter_mut()
        .find(|m| m.content.trim() == content)
    {
        existing.keywords = keywords.clone();
        existing.updated_at = now.clone();
        existing.id.clone()
    } else {
        let id = Uuid::new_v4().to_string();
        data.memories.push(MemoryEntry {
            id: id.clone(),
            content: content.to_string(),
            keywords: keywords.clone(),
            created_at: now.clone(),
            updated_at: now.clone(),
        });
        id
    };
    write_app_data(&app_state.data_path, &data)?;
    drop(guard);

    eprintln!(
        "[TOOL-DEBUG] memory-save saved. id={}, keywords={}, content_len={}, total_memories={}",
        memory_id,
        keywords.join(","),
        content.chars().count(),
        data.memories.len()
    );

    Ok(serde_json::json!({
      "saved": true,
      "id": memory_id,
      "keywords": keywords,
      "updatedAt": now
    }))
}

#[derive(Debug, Clone, Deserialize, Serialize)]
struct FetchToolArgs {
    url: String,
    #[serde(default)]
    max_length: Option<usize>,
}

#[derive(Debug, Clone, Deserialize, Serialize)]
struct BingSearchToolArgs {
    query: String,
    #[serde(default)]
    num_results: Option<usize>,
}

#[derive(Debug, Clone, Deserialize, Serialize)]
struct MemorySaveToolArgs {
    content: String,
    keywords: Vec<String>,
}

#[derive(Debug, Clone, Copy)]
struct BuiltinFetchTool;

impl Tool for BuiltinFetchTool {
    const NAME: &'static str = "fetch";
    type Error = ToolInvokeError;
    type Args = FetchToolArgs;
    type Output = Value;

    async fn definition(&self, _prompt: String) -> ToolDefinition {
        ToolDefinition {
            name: "fetch".to_string(),
            description: "Fetch webpage text.".to_string(),
            parameters: serde_json::json!({
              "type": "object",
              "properties": {
                "url": { "type": "string", "description": "URL" },
                "max_length": { "type": "integer", "description": "Max chars", "default": 1800 }
              },
              "required": ["url"]
            }),
        }
    }

    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {
        eprintln!(
            "[TOOL-DEBUG] execute_builtin_tool.start name=fetch args={}",
            debug_value_snippet(&serde_json::to_value(&args).unwrap_or(Value::Null), 240)
        );
        let result = builtin_fetch(&args.url, args.max_length.unwrap_or(1800))
            .await
            .map_err(ToolInvokeError::from);
        match &result {
            Ok(v) => eprintln!(
                "[TOOL-DEBUG] execute_builtin_tool.ok name=fetch result={}",
                debug_value_snippet(v, 240)
            ),
            Err(err) => eprintln!("[TOOL-DEBUG] execute_builtin_tool.err name=fetch err={err}"),
        }
        result
    }
}

#[derive(Debug, Clone, Copy)]
struct BuiltinBingSearchTool;

impl Tool for BuiltinBingSearchTool {
    const NAME: &'static str = "bing_search";
    type Error = ToolInvokeError;
    type Args = BingSearchToolArgs;
    type Output = Value;

    async fn definition(&self, _prompt: String) -> ToolDefinition {
        ToolDefinition {
            name: "bing_search".to_string(),
            description: "Search web with Bing.".to_string(),
            parameters: serde_json::json!({
              "type": "object",
              "properties": {
                "query": { "type": "string", "description": "Query" },
                "num_results": { "type": "integer", "description": "Result count", "default": 5 }
              },
              "required": ["query"]
            }),
        }
    }

    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {
        eprintln!(
            "[TOOL-DEBUG] execute_builtin_tool.start name=bing-search args={}",
            debug_value_snippet(&serde_json::to_value(&args).unwrap_or(Value::Null), 240)
        );
        let result = builtin_bing_search(&args.query, args.num_results.unwrap_or(5))
            .await
            .map_err(ToolInvokeError::from);
        match &result {
            Ok(v) => eprintln!(
                "[TOOL-DEBUG] execute_builtin_tool.ok name=bing-search result={}",
                debug_value_snippet(v, 240)
            ),
            Err(err) => {
                eprintln!("[TOOL-DEBUG] execute_builtin_tool.err name=bing-search err={err}")
            }
        }
        result
    }
}

#[derive(Debug, Clone)]
struct BuiltinMemorySaveTool {
    app_state: AppState,
}

impl Tool for BuiltinMemorySaveTool {
    const NAME: &'static str = "memory_save";
    type Error = ToolInvokeError;
    type Args = MemorySaveToolArgs;
    type Output = Value;

    async fn definition(&self, _prompt: String) -> ToolDefinition {
        ToolDefinition {
            name: "memory_save".to_string(),
            description: "保存与用户相关、长期有价值的记忆。禁止保存密码、密钥等敏感信息。"
                .to_string(),
            parameters: serde_json::json!({
              "type": "object",
              "properties": {
                "content": { "type": "string", "description": "记忆正文，简洁具体" },
                "keywords": {
                  "type": "array",
                  "items": { "type": "string" },
                  "description": "关键词列表，用于后续命中提示板"
                }
              },
              "required": ["content", "keywords"]
            }),
        }
    }

    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {
        let args_json = serde_json::json!({
            "content": args.content,
            "keywords": args.keywords,
        });
        eprintln!(
            "[TOOL-DEBUG] execute_builtin_tool.start name=memory-save args={}",
            debug_value_snippet(&args_json, 240)
        );
        let result = builtin_memory_save(&self.app_state, args_json).map_err(ToolInvokeError::from);
        match &result {
            Ok(v) => eprintln!(
                "[TOOL-DEBUG] execute_builtin_tool.ok name=memory-save result={}",
                debug_value_snippet(v, 240)
            ),
            Err(err) => {
                eprintln!("[TOOL-DEBUG] execute_builtin_tool.err name=memory-save err={err}")
            }
        }
        result
    }
}

fn openai_headers(api_key: &str) -> Result<HeaderMap, String> {
    let mut headers = HeaderMap::new();
    headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json"));
    let auth = format!("Bearer {}", api_key.trim());
    let auth_value = HeaderValue::from_str(&auth)
        .map_err(|err| format!("Build authorization header failed: {err}"))?;
    headers.insert(AUTHORIZATION, auth_value);
    Ok(headers)
}

fn candidate_openai_chat_urls(base_url: &str) -> Vec<String> {
    let base = base_url.trim().trim_end_matches('/');
    if base.is_empty() {
        return Vec::new();
    }
    let lower = base.to_ascii_lowercase();
    let mut urls = Vec::new();
    if lower.ends_with("/chat/completions") {
        urls.push(base.to_string());
    } else if lower.ends_with("/v1") {
        urls.push(format!("{base}/chat/completions"));
    } else {
        urls.push(format!("{base}/chat/completions"));
        urls.push(format!("{base}/v1/chat/completions"));
    }
    urls.sort();
    urls.dedup();
    urls
}

fn parse_stream_delta_text(content: &Option<Value>) -> String {
    match content {
        Some(Value::String(s)) => s.clone(),
        Some(Value::Array(items)) => items
            .iter()
            .filter_map(|it| it.get("text").and_then(Value::as_str))
            .collect::<Vec<_>>()
            .join(""),
        _ => String::new(),
    }
}

/// 通用 OpenAI SSE 流式请求：解析文本 delta（实时推送到 on_delta）和 tool_calls 积累。
/// 返回 (完整文本, 积累的 tool_calls)。
async fn openai_stream_request(
    client: &reqwest::Client,
    url: &str,
    body: Value,
    on_delta: &tauri::ipc::Channel<AssistantDeltaEvent>,
) -> Result<(String, Vec<OpenAIToolCall>), String> {
    openai_stream_request_with_sink(client, url, body, |delta| {
        let send_result = on_delta.send(AssistantDeltaEvent {
            delta: delta.to_string(),
            kind: None,
            tool_name: None,
            tool_status: None,
            message: None,
        });
        eprintln!(
            "[STREAM-DEBUG] on_delta.send result: {:?}, delta_len={}",
            send_result,
            delta.len()
        );
    })
    .await
}

async fn openai_stream_request_with_sink<F>(
    client: &reqwest::Client,
    url: &str,
    body: Value,
    mut on_delta: F,
) -> Result<(String, Vec<OpenAIToolCall>), String>
where
    F: FnMut(&str),
{
    eprintln!("[STREAM-DEBUG] openai_stream_request called, url={}", url);
    let resp = client
        .post(url)
        .json(&body)
        .send()
        .await
        .map_err(|err| format!("OpenAI stream request failed: {err}"))?;
    if !resp.status().is_success() {
        let status = resp.status();
        let raw = resp.text().await.unwrap_or_default();
        return Err(format!(
            "OpenAI stream failed with status {status}: {}",
            raw.chars().take(300).collect::<String>()
        ));
    }

    let mut stream = resp.bytes_stream();
    let mut buffer = String::new();
    let mut output = String::new();

    // 积累 tool_calls：按 index 分组
    let mut tool_calls_map: std::collections::BTreeMap<usize, (String, String, String)> =
        std::collections::BTreeMap::new(); // index -> (id, name, arguments)

    while let Some(item) = stream.next().await {
        let chunk = item.map_err(|err| format!("Read stream chunk failed: {err}"))?;
        let text = String::from_utf8_lossy(&chunk);
        buffer.push_str(&text);

        while let Some(pos) = buffer.find('\n') {
            let line = buffer[..pos].trim_end_matches('\r').to_string();
            buffer.drain(..=pos);

            if !line.starts_with("data:") {
                continue;
            }
            let data = line["data:".len()..].trim();
            if data.is_empty() {
                continue;
            }
            if data == "[DONE]" {
                break;
            }

            let Ok(parsed) = serde_json::from_str::<OpenAIStreamChunk>(data) else {
                eprintln!(
                    "[STREAM-DEBUG] SSE parse failed: {}",
                    &data[..data.len().min(200)]
                );
                continue;
            };
            if parsed.choices.is_empty() {
                continue;
            }
            let choice = &parsed.choices[0];

            // 处理文本 delta
            let delta_text = parse_stream_delta_text(&choice.delta.content);
            if !delta_text.is_empty() {
                output.push_str(&delta_text);
                on_delta(&delta_text);
            }

            // 处理 tool_calls delta
            if let Some(tc_deltas) = &choice.delta.tool_calls {
                for tc_delta in tc_deltas {
                    let entry = tool_calls_map
                        .entry(tc_delta.index)
                        .or_insert_with(|| (String::new(), String::new(), String::new()));
                    if let Some(id) = &tc_delta.id {
                        entry.0 = id.clone();
                    }
                    if let Some(func) = &tc_delta.function {
                        if let Some(name) = &func.name {
                            entry.1.push_str(name);
                        }
                        if let Some(args) = &func.arguments {
                            entry.2.push_str(args);
                        }
                    }
                }
            }
        }
    }

    let tool_calls: Vec<OpenAIToolCall> = tool_calls_map
        .into_iter()
        .map(|(index, (id, name, arguments))| OpenAIToolCall {
            id: if id.trim().is_empty() {
                format!("tool_call_{index}")
            } else {
                id
            },
            function: OpenAIToolCallFunction { name, arguments },
        })
        .filter(|tc| !tc.function.name.trim().is_empty())
        .collect();

    eprintln!(
        "[STREAM-DEBUG] stream done: output_len={}, tool_calls_count={}",
        output.len(),
        tool_calls.len()
    );
    Ok((output, tool_calls))
}

async fn call_model_openai_stream_text(
    api_config: &ResolvedApiConfig,
    model_name: &str,
    prepared: &PreparedPrompt,
    on_delta: &tauri::ipc::Channel<AssistantDeltaEvent>,
) -> Result<String, String> {
    let client = reqwest::Client::builder()
        .timeout(std::time::Duration::from_secs(120))
        .default_headers(openai_headers(&api_config.api_key)?)
        .build()
        .map_err(|err| format!("Build HTTP client failed: {err}"))?;
    let body = serde_json::json!({
      "model": model_name,
      "messages": [
        { "role": "system", "content": prepared.preamble },
        { "role": "user", "content": prepared.latest_user_text }
      ],
      "temperature": api_config.temperature,
      "stream": true
    });

    let urls = candidate_openai_chat_urls(&api_config.base_url);
    if urls.is_empty() {
        return Err("Base URL is empty.".to_string());
    }

    let mut errors = Vec::new();
    for url in urls {
        match openai_stream_request(&client, &url, body.clone(), on_delta).await {
            Ok((text, _)) => return Ok(text),
            Err(err) => errors.push(format!("{url} -> {err}")),
        }
    }

    Err(format!(
        "OpenAI stream request failed for all candidate URLs: {}",
        errors.join(" || ")
    ))
}

async fn call_model_openai_with_tools(
    api_config: &ResolvedApiConfig,
    selected_api: &ApiConfig,
    model_name: &str,
    prepared: PreparedPrompt,
    app_state: Option<&AppState>,
    on_delta: &tauri::ipc::Channel<AssistantDeltaEvent>,
    max_tool_iterations: usize,
) -> Result<String, String> {
    let has_fetch = tool_enabled(selected_api, "fetch");
    let has_bing = tool_enabled(selected_api, "bing-search");
    let has_memory = tool_enabled(selected_api, "memory-save");
    if !has_fetch && !has_bing && !has_memory {
        return call_model_openai_stream_text(api_config, model_name, &prepared, on_delta).await;
    }

    let mut client_builder: openai::ClientBuilder =
        openai::Client::builder().api_key(&api_config.api_key);
    if !api_config.base_url.is_empty() {
        client_builder = client_builder.base_url(&api_config.base_url);
    }
    let client = client_builder
        .build()
        .map_err(|err| format!("Failed to create OpenAI client via rig: {err}"))?;

    let mut tools: Vec<Box<dyn ToolDyn>> = Vec::new();
    if has_fetch {
        tools.push(Box::new(BuiltinFetchTool));
    }
    if has_bing {
        tools.push(Box::new(BuiltinBingSearchTool));
    }
    if has_memory {
        let state = app_state
            .ok_or_else(|| "memory_save requires app state".to_string())?
            .clone();
        tools.push(Box::new(BuiltinMemorySaveTool { app_state: state }));
    }

    let agent = client
        .clone()
        .completions_api()
        .agent(model_name)
        .preamble(&prepared.preamble)
        .temperature(api_config.temperature)
        .tools(tools)
        .build();

    let mut full_assistant_text = String::new();
    let mut current_prompt: RigMessage = prepared.latest_user_text.clone().into();
    let mut chat_history = Vec::<RigMessage>::new();

    for _ in 0..max_tool_iterations {
        let mut stream = agent
            .stream_completion(current_prompt.clone(), chat_history.clone())
            .await
            .map_err(|err| format!("rig stream completion build failed: {err}"))?
            .stream()
            .await
            .map_err(|err| format!("rig stream start failed: {err}"))?;

        chat_history.push(current_prompt.clone());

        let mut turn_text = String::new();
        let mut tool_calls = Vec::<AssistantContent>::new();
        let mut tool_results = Vec::<(String, Option<String>, String)>::new();
        let mut did_call_tool = false;

        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(StreamedAssistantContent::Text(text)) => {
                    let send_result = on_delta.send(AssistantDeltaEvent {
                        delta: text.text.clone(),
                        kind: None,
                        tool_name: None,
                        tool_status: None,
                        message: None,
                    });
                    eprintln!(
                        "[STREAM-DEBUG] on_delta.send result: {:?}, delta_len={}",
                        send_result,
                        text.text.len()
                    );
                    turn_text.push_str(&text.text);
                }
                Ok(StreamedAssistantContent::ToolCall {
                    tool_call,
                    internal_call_id: _,
                }) => {
                    did_call_tool = true;
                    eprintln!(
                        "[TOOL-DEBUG] tool_call id={} name={} args={}",
                        tool_call.id,
                        tool_call.function.name,
                        tool_call.function.arguments
                    );
                    send_tool_status_event(
                        on_delta,
                        &tool_call.function.name,
                        "running",
                        &format!("正在调用工具：{}", tool_call.function.name),
                    );
                    let tool_result = agent
                        .tool_server_handle
                        .call_tool(
                            &tool_call.function.name,
                            &tool_call.function.arguments.to_string(),
                        )
                        .await
                        .map_err(|err| {
                            send_tool_status_event(
                                on_delta,
                                &tool_call.function.name,
                                "failed",
                                &format!("工具调用失败：{} ({err})", tool_call.function.name),
                            );
                            format!("Tool call '{}' failed: {err}", tool_call.function.name)
                        })?;
                    eprintln!(
                        "[TOOL-DEBUG] tool_result id={} name={} content={}",
                        tool_call.id,
                        tool_call.function.name,
                        tool_result.chars().take(240).collect::<String>()
                    );
                    send_tool_status_event(
                        on_delta,
                        &tool_call.function.name,
                        "done",
                        &format!("工具调用完成：{}", tool_call.function.name),
                    );

                    tool_calls.push(AssistantContent::ToolCall(tool_call.clone()));
                    tool_results.push((tool_call.id, tool_call.call_id, tool_result));
                }
                Ok(StreamedAssistantContent::Final(_)) => {}
                Ok(StreamedAssistantContent::Reasoning(_)) => {}
                Ok(StreamedAssistantContent::ReasoningDelta { .. }) => {}
                Ok(StreamedAssistantContent::ToolCallDelta { .. }) => {}
                Err(err) => return Err(format!("rig streaming failed: {err}")),
            }
        }

        if !turn_text.is_empty() {
            if !full_assistant_text.trim().is_empty() {
                full_assistant_text.push_str("\n\n");
            }
            full_assistant_text.push_str(&turn_text);
        }

        if !did_call_tool {
            return Ok(full_assistant_text);
        }

        if !tool_calls.is_empty() {
            chat_history.push(RigMessage::Assistant {
                id: None,
                content: OneOrMany::many(tool_calls)
                    .map_err(|_| "Failed to build assistant tool-call message".to_string())?,
            });
        }

        for (tool_id, call_id, tool_result) in tool_results {
            let result_content = OneOrMany::one(ToolResultContent::text(tool_result));
            let user_content = if let Some(call_id) = call_id {
                UserContent::tool_result_with_call_id(tool_id, call_id, result_content)
            } else {
                UserContent::tool_result(tool_id, result_content)
            };
            chat_history.push(RigMessage::User {
                content: OneOrMany::one(user_content),
            });
        }

        current_prompt = chat_history
            .pop()
            .ok_or_else(|| "Tool call turn ended with empty chat history".to_string())?;
    }

    send_tool_status_event(
        on_delta,
        "tools",
        "failed",
        "工具调用达到上限，停止继续调用并立刻汇报。",
    );
    let final_instruction = "工具调用次数达到上限，必须立刻汇报。禁止继续调用任何工具。请基于已有信息直接给出结论，并明确不确定性。";
    current_prompt = final_instruction.into();
    let final_agent = client
        .completions_api()
        .agent(model_name)
        .preamble(&prepared.preamble)
        .temperature(api_config.temperature)
        .build();
    let mut final_stream = final_agent
        .stream_completion(current_prompt.clone(), chat_history.clone())
        .await
        .map_err(|err| format!("rig final stream build failed: {err}"))?
        .stream()
        .await
        .map_err(|err| format!("rig final stream start failed: {err}"))?;
    let mut final_text = String::new();
    while let Some(chunk) = final_stream.next().await {
        match chunk {
            Ok(StreamedAssistantContent::Text(text)) => {
                let send_result = on_delta.send(AssistantDeltaEvent {
                    delta: text.text.clone(),
                    kind: None,
                    tool_name: None,
                    tool_status: None,
                    message: None,
                });
                eprintln!(
                    "[STREAM-DEBUG] on_delta.send result: {:?}, delta_len={}",
                    send_result,
                    text.text.len()
                );
                final_text.push_str(&text.text);
            }
            Ok(StreamedAssistantContent::Final(_)) => {}
            Ok(StreamedAssistantContent::Reasoning(_)) => {}
            Ok(StreamedAssistantContent::ReasoningDelta { .. }) => {}
            Ok(StreamedAssistantContent::ToolCall { .. }) => {}
            Ok(StreamedAssistantContent::ToolCallDelta { .. }) => {}
            Err(err) => return Err(format!("rig final streaming failed: {err}")),
        }
    }
    if !final_text.trim().is_empty() {
        if !full_assistant_text.trim().is_empty() {
            full_assistant_text.push_str("\n\n");
        }
        full_assistant_text.push_str(&final_text);
    }
    Ok(full_assistant_text)
}

async fn call_model_openai_style(
    api_config: &ResolvedApiConfig,
    selected_api: &ApiConfig,
    model_name: &str,
    prepared: PreparedPrompt,
    app_state: Option<&AppState>,
    on_delta: &tauri::ipc::Channel<AssistantDeltaEvent>,
    max_tool_iterations: usize,
) -> Result<String, String> {
    eprintln!(
        "[STREAM-DEBUG] call_model_openai_style: format={}, enable_tools={}, images={}, audios={}",
        selected_api.request_format,
        selected_api.enable_tools,
        prepared.latest_images.len(),
        prepared.latest_audios.len()
    );
    // 优先使用工具调用（如果启用）
    if selected_api.enable_tools
        && is_openai_style_request_format(&selected_api.request_format)
        && prepared.latest_images.is_empty()
        && prepared.latest_audios.is_empty()
    {
        return call_model_openai_with_tools(
            api_config,
            selected_api,
            model_name,
            prepared,
            app_state,
            on_delta,
            max_tool_iterations,
        )
        .await;
    }

    // 纯文本流式传输（无论工具是否启用，只要没有工具调用就走流式）
    if is_openai_style_request_format(&selected_api.request_format)
        && prepared.latest_images.is_empty()
        && prepared.latest_audios.is_empty()
    {
        return call_model_openai_stream_text(api_config, model_name, &prepared, on_delta).await;
    }

    // 回退到 rig（支持多模态）
    let original = prepared.clone();
    let rig_result = call_model_openai_rig_style(api_config, model_name, prepared).await;
    match rig_result {
        Ok(text) => Ok(text),
        Err(err)
            if !original.latest_images.is_empty() && is_image_unsupported_error(&err) =>
        {
            eprintln!(
                "[CHAT] Model rejected image input, fallback to text-only request. error={}",
                err
            );
            let mut fallback = original;
            fallback.latest_images.clear();
            fallback.latest_audios.clear();
            call_model_openai_stream_text(api_config, model_name, &fallback, on_delta).await
        }
        Err(err) => Err(err),
    }
}

async fn describe_image_with_vision_api(
    vision_resolved: &ResolvedApiConfig,
    vision_api: &ApiConfig,
    image: &BinaryPart,
) -> Result<String, String> {
    if !is_openai_style_request_format(&vision_resolved.request_format) {
        return Err(format!(
            "Vision request format '{}' is not implemented yet.",
            vision_resolved.request_format
        ));
    }

    let mime = image.mime.trim();
    let prepared = PreparedPrompt {
        preamble: "[SYSTEM PROMPT]\n你是图像理解助手。请读取图片中的关键信息并输出简洁中文描述，保留有价值的文本、数字、UI元素与上下文。".to_string(),
        latest_user_text: "请识别这张图片并给出可用于后续对话的文本描述。".to_string(),
        latest_images: vec![(
            if mime.is_empty() {
                "image/png".to_string()
            } else {
                mime.to_string()
            },
            image.bytes_base64.clone(),
        )],
        latest_audios: Vec::new(),
    };

    call_model_openai_rig_style(vision_resolved, &vision_api.model, prepared).await
}


