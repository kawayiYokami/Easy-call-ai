#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]

use std::{
    fs,
    io::Cursor,
    path::PathBuf,
    sync::{Arc, Mutex, OnceLock},
};

use base64::{engine::general_purpose::STANDARD as B64, Engine as _};
use directories::ProjectDirs;
use futures_util::StreamExt;
use image::ImageFormat;
use reqwest::header::{HeaderMap, HeaderValue, AUTHORIZATION, CONTENT_TYPE};
use rig::{
    completion::{
        message::{AudioMediaType, ImageDetail, ImageMediaType, UserContent},
        Message as RigMessage, Prompt, ToolDefinition,
    },
    message::{AssistantContent, ToolResultContent},
    prelude::CompletionClient,
    providers::openai,
    streaming::{StreamedAssistantContent, StreamingCompletion},
    tool::{Tool, ToolDyn},
    OneOrMany,
};
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use tauri::{
    menu::{Menu, MenuItem},
    tray::TrayIconBuilder,
    AppHandle, Emitter, Manager, PhysicalPosition, Position, State,
};
use tauri_plugin_dialog::DialogExt;
use tauri_plugin_global_shortcut::{GlobalShortcutExt, Shortcut, ShortcutState};
use time::{format_description::well_known::Rfc3339, OffsetDateTime};
use uuid::Uuid;


// ==================== 核心领域模型 ====================
include!("features/core/domain.rs");

// ==================== 配置与存储 ====================
include!("features/config/storage_and_stt.rs");

// ==================== 对话核心 ====================
include!("features/chat/conversation.rs");
include!("features/chat/model_runtime.rs");

// ==================== 系统窗口与命令 ====================
include!("features/system/windowing.rs");

// ==================== 记忆匹配 ====================
include!("features/memory/matcher.rs");

include!("features/system/commands.rs");

fn main() {
    let state = match AppState::new() {
        Ok(state) => state,
        Err(err) => {
            eprintln!("Failed to initialize application state: {err}");
            return;
        }
    };

    tauri::Builder::default()
        .plugin(tauri_plugin_dialog::init())
        .plugin(
            tauri_plugin_global_shortcut::Builder::new()
                .with_handler(|app, _shortcut, event| {
                    if event.state() == ShortcutState::Pressed {
                        let _ = toggle_window(app, "chat");
                    }
                })
                .build(),
        )
        .manage(state)
        .setup(|app| {
            let app_handle = app.handle().clone();
            register_default_hotkey(&app_handle)?;
            build_tray(&app_handle)?;
            let app_state = app_handle.state::<AppState>();
            let guard = app_state
                .state_lock
                .lock()
                .map_err(|_| "Failed to lock state mutex".to_string())?;
            let mut data = read_app_data(&app_state.data_path).unwrap_or_default();
            let changed = ensure_default_agent(&mut data);
            if changed {
                let _ = write_app_data(&app_state.data_path, &data);
            }
            let avatar_path = data
                .agents
                .iter()
                .find(|a| a.id == data.selected_agent_id)
                .and_then(|a| a.avatar_path.clone());
            drop(guard);
            let _ = sync_tray_icon_from_avatar_path(&app_handle, avatar_path.as_deref());
            hide_on_close(&app_handle);
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            load_config,
            save_config,
            load_agents,
            save_agents,
            load_chat_settings,
            save_chat_settings,
            save_agent_avatar,
            clear_agent_avatar,
            read_avatar_data_url,
            sync_tray_icon,
            save_conversation_api_settings,
            get_chat_snapshot,
            get_active_conversation_messages,
            get_prompt_preview,
            get_system_prompt_preview,
            list_archives,
            list_memories,
            export_memories,
            export_memories_to_file,
            export_memories_to_path,
            import_memories,
            get_archive_messages,
            delete_archive,
            export_archive_to_file,
            open_external_url,
            send_chat_message,
            force_archive_current,
            refresh_models,
            check_tools_status,
            get_image_text_cache_stats,
            clear_image_text_cache,
            send_debug_probe
        ])
        .run(tauri::generate_context!())
        .unwrap_or_else(|err| {
            eprintln!("error while running tauri application: {err}");
        });
}

#[cfg(test)]
mod tests {
    use super::*;
    use httpmock::{
        Method::{GET, POST},
        MockServer,
    };

    fn test_runtime() -> tokio::runtime::Runtime {
        tokio::runtime::Builder::new_current_thread()
            .enable_all()
            .build()
            .expect("build tokio runtime")
    }

    #[test]
    fn candidate_openai_chat_urls_should_handle_common_forms() {
        assert_eq!(
            candidate_openai_chat_urls("https://api.openai.com/v1"),
            vec!["https://api.openai.com/v1/chat/completions".to_string()]
        );
        assert_eq!(
            candidate_openai_chat_urls("https://gateway.example.com/chat/completions"),
            vec!["https://gateway.example.com/chat/completions".to_string()]
        );
        assert_eq!(
            candidate_openai_chat_urls("https://gateway.example.com"),
            vec![
                "https://gateway.example.com/chat/completions".to_string(),
                "https://gateway.example.com/v1/chat/completions".to_string()
            ]
        );
        assert!(candidate_openai_chat_urls("  ").is_empty());
    }

    #[test]
    fn image_text_cache_upsert_and_find_should_work() {
        let mut data = AppData::default();
        upsert_image_text_cache(&mut data, "h1", "vision-a", "text-a");
        assert_eq!(
            find_image_text_cache(&data, "h1", "vision-a"),
            Some("text-a".to_string())
        );

        upsert_image_text_cache(&mut data, "h1", "vision-a", "text-b");
        assert_eq!(
            find_image_text_cache(&data, "h1", "vision-a"),
            Some("text-b".to_string())
        );
        assert_eq!(find_image_text_cache(&data, "h1", "vision-b"), None);
    }

    #[test]
    fn compute_image_hash_hex_should_be_stable() {
        let png_1x1_red = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMCAO9WfXkAAAAASUVORK5CYII=";
        let part = BinaryPart {
            mime: "image/png".to_string(),
            bytes_base64: png_1x1_red.to_string(),
        };
        let h1 = compute_image_hash_hex(&part).expect("hash1");
        let h2 = compute_image_hash_hex(&part).expect("hash2");
        assert_eq!(h1, h2);
        assert!(!h1.is_empty());
    }

    #[test]
    fn normalize_app_config_should_fix_invalid_record_and_stt_fields() {
        let mut cfg = AppConfig {
            hotkey: "Alt+·".to_string(),
            ui_language: default_ui_language(),
            record_hotkey: "".to_string(),
            min_record_seconds: 0,
            max_record_seconds: 0,
            tool_max_iterations: 0,
            selected_api_config_id: "a1".to_string(),
            chat_api_config_id: "a1".to_string(),
            vision_api_config_id: None,
            api_configs: vec![
                ApiConfig {
                    id: "a1".to_string(),
                    name: "chat".to_string(),
                    request_format: "openai".to_string(),
                    enable_text: true,
                    enable_image: true,
                    enable_audio: false,
                    enable_tools: false,
                    tools: vec![],
                    base_url: "https://api.openai.com/v1".to_string(),
                    api_key: "k".to_string(),
                    model: "m".to_string(),
                    temperature: 1.0,
                    context_window_tokens: 128_000,
                },
                ApiConfig {
                    id: "a2".to_string(),
                    name: "bad-stt".to_string(),
                    request_format: "openai".to_string(),
                    enable_text: true,
                    enable_image: false,
                    enable_audio: true,
                    enable_tools: false,
                    tools: vec![],
                    base_url: "https://api.openai.com/v1".to_string(),
                    api_key: "k".to_string(),
                    model: "m".to_string(),
                    temperature: 1.0,
                    context_window_tokens: 128_000,
                },
            ],
        };
        normalize_app_config(&mut cfg);
        assert_eq!(cfg.record_hotkey, "Alt");
        assert_eq!(cfg.min_record_seconds, 1);
        assert!(cfg.max_record_seconds >= cfg.min_record_seconds);
        assert_eq!(cfg.tool_max_iterations, 1);
    }

    #[test]
    fn normalize_app_config_should_not_bind_chat_api_to_selected_api() {
        let mut cfg = AppConfig {
            hotkey: "Alt+·".to_string(),
            ui_language: default_ui_language(),
            record_hotkey: "Alt".to_string(),
            min_record_seconds: 1,
            max_record_seconds: 60,
            tool_max_iterations: 10,
            selected_api_config_id: "edit-b".to_string(),
            chat_api_config_id: "chat-a".to_string(),
            vision_api_config_id: None,
            api_configs: vec![
                ApiConfig {
                    id: "chat-a".to_string(),
                    name: "chat-a".to_string(),
                    request_format: "openai".to_string(),
                    enable_text: true,
                    enable_image: true,
                    enable_audio: true,
                    enable_tools: false,
                    tools: vec![],
                    base_url: "https://api.openai.com/v1".to_string(),
                    api_key: "k".to_string(),
                    model: "m".to_string(),
                    temperature: 1.0,
                    context_window_tokens: 128_000,
                },
                ApiConfig {
                    id: "edit-b".to_string(),
                    name: "edit-b".to_string(),
                    request_format: "openai".to_string(),
                    enable_text: true,
                    enable_image: false,
                    enable_audio: false,
                    enable_tools: false,
                    tools: vec![],
                    base_url: "https://api.openai.com/v1".to_string(),
                    api_key: "k".to_string(),
                    model: "m".to_string(),
                    temperature: 1.0,
                    context_window_tokens: 128_000,
                },
            ],
        };
        normalize_app_config(&mut cfg);
        assert_eq!(cfg.selected_api_config_id, "edit-b".to_string());
        assert_eq!(cfg.chat_api_config_id, "chat-a".to_string());
    }

    #[test]
    fn normalize_app_config_should_disable_audio_capability_globally() {
        let mut cfg = AppConfig {
            hotkey: "Alt+·".to_string(),
            ui_language: default_ui_language(),
            record_hotkey: "Alt".to_string(),
            min_record_seconds: 1,
            max_record_seconds: 60,
            tool_max_iterations: 10,
            selected_api_config_id: "tts-a".to_string(),
            chat_api_config_id: "tts-a".to_string(),
            vision_api_config_id: Some("tts-a".to_string()),
            api_configs: vec![ApiConfig {
                id: "tts-a".to_string(),
                name: "tts-a".to_string(),
                request_format: "openai_tts".to_string(),
                enable_text: true,
                enable_image: false,
                enable_audio: true,
                enable_tools: true,
                tools: vec![],
                base_url: "https://api.siliconflow.cn/v1/audio/transcriptions".to_string(),
                api_key: "k".to_string(),
                model: "m".to_string(),
                temperature: 1.0,
                context_window_tokens: 128_000,
            }],
        };
        normalize_app_config(&mut cfg);
        let api = &cfg.api_configs[0];
        assert!(api.enable_text);
        assert!(!api.enable_image);
        assert!(!api.enable_audio);
        assert!(api.enable_tools);
        assert_eq!(cfg.vision_api_config_id, None);
    }

    #[test]
    fn fetch_models_openai_should_read_models_from_base_url() {
        let server = MockServer::start();
        let model_mock = server.mock(|when, then| {
            when.method(GET).path("/models");
            then.status(200).json_body(serde_json::json!({
              "data": [
                { "id": "gpt-4o-mini" },
                { "id": "gpt-4.1-mini" }
              ]
            }));
        });

        let input = RefreshModelsInput {
            base_url: server.base_url(),
            api_key: "test-key".to_string(),
            request_format: "openai".to_string(),
        };

        let rt = test_runtime();
        let models = rt
            .block_on(fetch_models_openai(&input))
            .expect("fetch models from mock");

        model_mock.assert();
        assert_eq!(
            models,
            vec!["gpt-4.1-mini".to_string(), "gpt-4o-mini".to_string()]
        );
    }

    #[test]
    fn fetch_models_openai_should_fallback_to_v1_models() {
        let server = MockServer::start();
        let base_404_mock = server.mock(|when, then| {
            when.method(GET).path("/models");
            then.status(404).body("not found");
        });
        let v1_ok_mock = server.mock(|when, then| {
            when.method(GET).path("/v1/models");
            then.status(200).json_body(serde_json::json!({
              "data": [{ "id": "moonshot-v1-8k" }]
            }));
        });

        let input = RefreshModelsInput {
            base_url: server.base_url(),
            api_key: "test-key".to_string(),
            request_format: "openai".to_string(),
        };

        let rt = test_runtime();
        let models = rt
            .block_on(fetch_models_openai(&input))
            .expect("fallback /v1/models should succeed");

        base_404_mock.assert();
        v1_ok_mock.assert();
        assert_eq!(models, vec!["moonshot-v1-8k".to_string()]);
    }

    #[test]
    fn openai_stream_request_with_sink_should_emit_incremental_deltas() {
        let server = MockServer::start();
        let sse_body = concat!(
            "data: {\"choices\":[{\"delta\":{\"content\":\"你\"}}]}\n",
            "\n",
            "data: {\"choices\":[{\"delta\":{\"content\":\"好\"}}]}\n",
            "\n",
            "data: [DONE]\n",
            "\n"
        );
        let sse_mock = server.mock(|when, then| {
            when.method(POST).path("/v1/chat/completions");
            then.status(200)
                .header("content-type", "text/event-stream")
                .body(sse_body);
        });

        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(10))
            .build()
            .expect("build reqwest client");
        let body = serde_json::json!({
          "model": "gpt-4o-mini",
          "messages": [{ "role": "user", "content": "hello" }],
          "stream": true
        });

        let mut deltas = Vec::<String>::new();
        let rt = test_runtime();
        let (full_text, reasoning_standard, reasoning_inline, tool_calls) = rt
            .block_on(openai_stream_request_with_sink(
                &client,
                &format!("{}/v1/chat/completions", server.base_url()),
                body,
                |kind, delta| {
                    if kind == "text" {
                        deltas.push(delta.to_string());
                    }
                },
            ))
            .expect("stream request should parse");

        sse_mock.assert();
        assert_eq!(deltas, vec!["你".to_string(), "好".to_string()]);
        assert_eq!(full_text, "你好".to_string());
        assert!(reasoning_standard.is_empty());
        assert!(reasoning_inline.is_empty());
        assert!(tool_calls.is_empty());
    }

    #[test]
    fn openai_stream_request_with_sink_should_assemble_tool_calls_from_fragments() {
        let server = MockServer::start();
        let sse_body = concat!(
      "data: {\"choices\":[{\"delta\":{\"tool_calls\":[{\"index\":0,\"id\":\"call_1\",\"function\":{\"name\":\"bing_\",\"arguments\":\"{\\\"query\\\":\\\"\"}}]}}]}\n",
      "\n",
      "data: {\"choices\":[{\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"search\",\"arguments\":\"rust\\\"}\"}}]}}]}\n",
      "\n",
      "data: [DONE]\n",
      "\n"
    );
        let sse_mock = server.mock(|when, then| {
            when.method(POST).path("/v1/chat/completions");
            then.status(200)
                .header("content-type", "text/event-stream")
                .body(sse_body);
        });

        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(10))
            .build()
            .expect("build reqwest client");
        let body = serde_json::json!({
          "model": "gpt-4o-mini",
          "messages": [{ "role": "user", "content": "hello" }],
          "stream": true
        });

        let rt = test_runtime();
        let (_full_text, _reasoning_standard, _reasoning_inline, tool_calls) = rt
            .block_on(openai_stream_request_with_sink(
                &client,
                &format!("{}/v1/chat/completions", server.base_url()),
                body,
                |_kind, _delta| {},
            ))
            .expect("stream tool call should parse");

        sse_mock.assert();
        assert_eq!(tool_calls.len(), 1);
        assert_eq!(tool_calls[0].id, "call_1".to_string());
        assert_eq!(tool_calls[0].function.name, "bing_search".to_string());
        assert_eq!(
            tool_calls[0].function.arguments,
            "{\"query\":\"rust\"}".to_string()
        );
    }

    fn test_text_message(role: &str, text: &str, created_at: &str) -> ChatMessage {
        ChatMessage {
            id: Uuid::new_v4().to_string(),
            role: role.to_string(),
            created_at: created_at.to_string(),
            parts: vec![MessagePart::Text {
                text: text.to_string(),
            }],
            extra_text_blocks: Vec::new(),
            provider_meta: None,
            tool_call: None,
            mcp_call: None,
        }
    }

    fn test_active_conversation_with_messages(
        messages: Vec<ChatMessage>,
        last_user_at: Option<String>,
    ) -> Conversation {
        let now = now_iso();
        Conversation {
            id: Uuid::new_v4().to_string(),
            title: "t".to_string(),
            api_config_id: "api".to_string(),
            agent_id: "agent".to_string(),
            created_at: now.clone(),
            updated_at: now,
            last_user_at,
            last_assistant_at: None,
            last_context_usage_ratio: 0.0,
            status: "active".to_string(),
            messages,
        }
    }

    #[test]
    fn build_prompt_should_include_structured_tool_history_messages() {
        let now = now_iso();
        let mut assistant_with_tool = test_text_message("assistant", "我去查一下", &now);
        assistant_with_tool.tool_call = Some(vec![
            serde_json::json!({
                "role": "assistant",
                "content": null,
                "tool_calls": [{
                    "id": "call_1",
                    "type": "function",
                    "function": {
                        "name": "bing_search",
                        "arguments": "{\"query\":\"rust\"}"
                    }
                }]
            }),
            serde_json::json!({
                "role": "tool",
                "tool_call_id": "call_1",
                "content": "{\"results\":[{\"title\":\"Rust\"}]}"
            }),
        ]);

        let messages = vec![
            test_text_message("user", "帮我查 Rust", &now),
            assistant_with_tool,
            test_text_message("user", "继续", &now),
        ];
        let conv = test_active_conversation_with_messages(messages, Some(now));
        let agent = default_agent();

        let prepared = build_prompt(
            &conv,
            &agent,
            "用户",
            "我是...",
            DEFAULT_RESPONSE_STYLE_ID,
            "zh-CN",
        );

        assert!(
            prepared
                .history_messages
                .iter()
                .any(|m| m.role == "assistant" && m.tool_calls.is_some())
        );
        assert!(
            prepared.history_messages.iter().any(|m| {
                m.role == "tool"
                    && m.tool_call_id.as_deref() == Some("call_1")
                    && m.text.contains("\"results\"")
            })
        );
    }

    #[test]
    fn request_preview_should_keep_structured_tool_history_messages() {
        let api = ApiConfig {
            id: "api-a".to_string(),
            name: "api-a".to_string(),
            request_format: "openai".to_string(),
            enable_text: true,
            enable_image: false,
            enable_audio: false,
            enable_tools: true,
            tools: default_api_tools(),
            base_url: "https://example.com/v1".to_string(),
            api_key: "k".to_string(),
            model: "gpt-x".to_string(),
            temperature: 0.7,
            context_window_tokens: 128_000,
        };
        let prepared = PreparedPrompt {
            preamble: "sys".to_string(),
            history_messages: vec![
                PreparedHistoryMessage {
                    role: "assistant".to_string(),
                    text: String::new(),
                    tool_calls: Some(vec![serde_json::json!({
                        "id": "call_1",
                        "type": "function",
                        "function": { "name": "bing_search", "arguments": "{\"query\":\"rust\"}" }
                    })]),
                    tool_call_id: None,
                    reasoning_content: None,
                },
                PreparedHistoryMessage {
                    role: "tool".to_string(),
                    text: "{\"results\":[{\"title\":\"Rust\"}]}".to_string(),
                    tool_calls: None,
                    tool_call_id: Some("call_1".to_string()),
                    reasoning_content: None,
                },
            ],
            latest_user_text: "继续".to_string(),
            latest_user_system_text: "<time_context><utc>2026-02-11T17:30:45Z</utc></time_context>"
                .to_string(),
            latest_images: Vec::new(),
            latest_audios: Vec::new(),
        };
        let preview = build_request_preview_value(
            &api,
            &prepared,
            vec![
                serde_json::json!({"type":"text","text":"继续"}),
                serde_json::json!({"type":"text","text":prepared.latest_user_system_text}),
            ],
        );
        let messages = preview
            .get("messages")
            .and_then(Value::as_array)
            .expect("messages array");
        assert!(messages.iter().any(|m| {
            m.get("role").and_then(Value::as_str) == Some("assistant")
                && m.get("tool_calls").and_then(Value::as_array).is_some()
        }));
        assert!(messages.iter().any(|m| {
            m.get("role").and_then(Value::as_str) == Some("tool")
                && m.get("tool_call_id").and_then(Value::as_str) == Some("call_1")
        }));
    }

    #[test]
    fn archive_decision_should_force_when_usage_reaches_82pct() {
        let now = now_iso();
        let huge = "中".repeat(2000);
        let conv = test_active_conversation_with_messages(
            vec![test_text_message("user", &huge, &now)],
            Some(now),
        );
        let d = decide_archive_before_user_message(&conv, 1000);
        assert!(d.should_archive);
        assert!(d.forced);
        assert!(d.usage_ratio >= 0.82);
    }

    #[test]
    fn archive_decision_should_archive_after_30m_and_30pct() {
        let now = now_utc();
        let old = (now - time::Duration::minutes(31))
            .format(&Rfc3339)
            .expect("format old time");
        let text = "中".repeat(600);
        let conv = test_active_conversation_with_messages(
            vec![test_text_message("user", &text, &old)],
            Some(old),
        );
        let d = decide_archive_before_user_message(&conv, 1000);
        assert!(d.should_archive);
        assert!(!d.forced);
        assert!(d.usage_ratio >= 0.30);
    }

    #[test]
    fn archive_decision_should_not_archive_when_usage_below_30pct() {
        let now = now_utc();
        let old = (now - time::Duration::minutes(31))
            .format(&Rfc3339)
            .expect("format old time");
        let conv = test_active_conversation_with_messages(
            vec![test_text_message("user", "hello", &old)],
            Some(old),
        );
        let d = decide_archive_before_user_message(&conv, 1000);
        assert!(!d.should_archive);
        assert!(!d.forced);
        assert!(d.usage_ratio < 0.30);
    }

    fn count_xml_tag(xml: &str, tag: &str) -> usize {
        let needle = format!("<{}>", tag);
        xml.match_indices(&needle).count()
    }

    #[test]
    fn memory_board_should_match_user_text_only_and_require_hit() {
        let now = now_iso();
        let conv = test_active_conversation_with_messages(
            vec![
                test_text_message("user", "hello world", &now),
                test_text_message("assistant", "k99 only assistant side", &now),
            ],
            Some(now),
        );
        let search_text = conversation_search_text(&conv);
        assert!(search_text.contains("hello world"));
        assert!(!search_text.contains("k99 only assistant side"));

        let memories = vec![
            MemoryEntry {
                id: "m-user".to_string(),
                content: "user-hit".to_string(),
                keywords: vec!["hello".to_string()],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m-assistant-only".to_string(),
                content: "assistant-only-hit".to_string(),
                keywords: vec!["k99".to_string()],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
        ];

        let xml =
            build_memory_board_xml(&memories, &search_text, "").expect("should have one hit");
        assert!(xml.contains("<content>user-hit</content>"));
        assert!(!xml.contains("assistant-only-hit"));
    }

    #[test]
    fn memory_board_should_sort_by_hit_count_and_cap_at_seven() {
        let now = now_iso();
        let user_text =
            "k01 k02 k03 k04 k05 k06 k07 k08 k09 k10 k11 k12 k13 k14 k15 k16".to_string();
        let conv = test_active_conversation_with_messages(
            vec![test_text_message("user", &user_text, &now)],
            Some(now),
        );
        let search_text = conversation_search_text(&conv);

        let memories = vec![
            MemoryEntry {
                id: "m1".to_string(),
                content: "rank-8".to_string(),
                keywords: vec![
                    "k01".to_string(),
                    "k02".to_string(),
                    "k03".to_string(),
                    "k04".to_string(),
                    "k05".to_string(),
                    "k06".to_string(),
                    "k07".to_string(),
                    "k08".to_string(),
                ],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m2".to_string(),
                content: "rank-7".to_string(),
                keywords: vec![
                    "k01".to_string(),
                    "k02".to_string(),
                    "k03".to_string(),
                    "k04".to_string(),
                    "k05".to_string(),
                    "k06".to_string(),
                    "k07".to_string(),
                ],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m3".to_string(),
                content: "rank-6".to_string(),
                keywords: vec![
                    "k01".to_string(),
                    "k02".to_string(),
                    "k03".to_string(),
                    "k04".to_string(),
                    "k05".to_string(),
                    "k06".to_string(),
                ],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m4".to_string(),
                content: "rank-5".to_string(),
                keywords: vec![
                    "k01".to_string(),
                    "k02".to_string(),
                    "k03".to_string(),
                    "k04".to_string(),
                    "k05".to_string(),
                ],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m5".to_string(),
                content: "rank-4".to_string(),
                keywords: vec![
                    "k01".to_string(),
                    "k02".to_string(),
                    "k03".to_string(),
                    "k04".to_string(),
                ],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m6".to_string(),
                content: "rank-3".to_string(),
                keywords: vec!["k01".to_string(), "k02".to_string(), "k03".to_string()],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m7".to_string(),
                content: "rank-2".to_string(),
                keywords: vec!["k01".to_string(), "k02".to_string()],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
            MemoryEntry {
                id: "m8".to_string(),
                content: "rank-1".to_string(),
                keywords: vec!["k01".to_string()],
                created_at: now_iso(),
                updated_at: now_iso(),
            },
        ];

        let xml =
            build_memory_board_xml(&memories, &search_text, "").expect("should produce board");

        assert_eq!(count_xml_tag(&xml, "memory"), 7);
        assert!(xml.contains("<content>rank-8</content>"));
        assert!(xml.contains("<content>rank-2</content>"));
        assert!(!xml.contains("rank-1"));

        let idx_rank_8 = xml.find("rank-8").expect("rank-8 index");
        let idx_rank_2 = xml.find("rank-2").expect("rank-2 index");
        assert!(idx_rank_8 < idx_rank_2);
    }
}
